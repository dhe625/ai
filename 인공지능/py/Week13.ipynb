{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse relationship between precision & recall\n",
    "\n",
    "# F measure : harmonic mean of precision & recall\n",
    "# F = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "# F measure 공식의 과정\n",
    "# Geometric mean = √x1*x2\n",
    "# Harmony = G^2 / Arithmetic mean\n",
    "# H = x1 * x2 / ((x1+x2)/2)\n",
    "\n",
    "# p.8 예제 중요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold Cross-validation : Randomly partition the data into k mutually exclusive subsets, each approximately equal size\n",
    "# - Leave-one-out : k folds where k = # of tuples, for small sized data\n",
    "\n",
    "# Holdout Method : data set에서 임의로 test set을 지정\n",
    "# - Given data is randomly partitioned into two independent sets\n",
    "\n",
    "    # Holdout Cross Validation (Random subsampling)\n",
    "    # - Repeat holdout k times\n",
    "    # partition을 나누는 기준을 random하게 함\n",
    "\n",
    "# Stratified cross-validation - data가 편향되어 있을 경우\n",
    "# fold를 일정하게 유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time(seconds) 2.39\n",
      "각 validation 정답률 =  [0.68511685 1.         1.         1.         1.         1.\n",
      " 1.         1.         0.99630542 1.        ]\n",
      "평균 정답률 : 0.97\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, model_selection\n",
    "from time import time\n",
    "\n",
    "mr = pd.read_csv(\"/Volumes/GoogleDrive-107262488266475120044/내 드라이브/3-1/인공지능/py/mushroom.csv\", header = None)\n",
    "\n",
    "df = pd.DataFrame(mr.iloc[:, 0])\n",
    "df = df.join(pd.get_dummies(mr.iloc[:, 1: ]))\n",
    "\n",
    "data = df.iloc[:, 1:]\n",
    "label = df.loc[:, 0]\n",
    "\n",
    "start = time()\n",
    "\n",
    "# random vectors == base classifiers(decision trees)\n",
    "clf = RandomForestClassifier(n_estimators = 100)\n",
    "clf.fit(data, label)\n",
    "\n",
    "scores = model_selection.cross_val_score(clf, data, label, cv = 10)\n",
    "\n",
    "end = time()\n",
    "\n",
    "print(\"Execution time(seconds)\", str(round((end-start), 2)))\n",
    "print(\"각 validation 정답률 = \", scores)\n",
    "print(\"평균 정답률 :\", \"%.2f\"%scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "# - Supervised learning\n",
    "# - predict results within a \"continuous\" output\n",
    "# - map input variables to some continuous function\n",
    "\n",
    "# h𝜃(𝑥) = 𝜃0 + 𝜃1𝑥\n",
    "# h𝜃(𝑥) is hypothesis\n",
    "# 𝜃 is parameters\n",
    "\n",
    "# Univariate linear regression (단변량 선형회귀) : Linear regression with one variable\n",
    "\n",
    "# Cost function : 오차를 측정하여 가장 작은 값 채택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p.11\n",
    "\n",
    "# Squared error function - 가장 대표적인 cost function의 예\n",
    "# 𝐽(𝜃0,𝜃1) = Σ(h(x)-y)^2 / 2m (예측 값 h𝜃(𝑥)와 실제 값 y간의 간격이 가장 작은 𝜃값을 채택하기 위함)\n",
    "# 𝐽(𝜃0,𝜃1)이 가장 작은 값을 채택\n",
    "\n",
    "# [ Gradient Descent algorithm ] : An algorithm for minimizing the cost function J for the linear regression\n",
    "# - iterative algorithm\n",
    "# - random한 초기값에서 주변을 둘러보면서 더 작은 값을 찾아가며 minimum 탐색\n",
    "# - 초기값이 random하므로 local minimum을 찾아내고, global minimum은 찾지 못할 수 있음\n",
    "\n",
    "# p.14\n",
    "# := : 할당 연산자\n",
    "# ⍺ : learning rate (학습하는 비율)\n",
    "# 𝜃j := 𝜃j - ⍺ * slope(기울기)\n",
    "# Simultaneously update 𝜃0 and 𝜃1\n",
    "\n",
    "# p.16 - bowl shape\n",
    "# 반복하면 minimum 쪽으로 수렴하게됨\n",
    "\n",
    "# Problems\n",
    "# 1. ⍺가 너무 작으면 더디게 내려가고, ⍺가 너무 크면 지나친다\n",
    "# 2. Local minimum problem - 𝜃j is stucked at Local minimum\n",
    "#    - Squared error function은 bowl shape으로 만들어줌 -> local minimum problem x\n",
    "# ⍺를 fixed 하더라도 slope가 반복할수록 점점 작아지므로 diverge하지 않음\n",
    "\n",
    "# p.22 공식 중요\n",
    "# Batch : Each step of gradient descent uses all the training examples.\n",
    "\n",
    "# Convex function\n",
    "# - Cost function for linear regression\n",
    "# - 아래에서 봤을 때 움푹 파인 곳이 없음\n",
    "# - global minimum만 존재\n",
    "\n",
    "# Gradient Descent는 scalability가 좋음 -> 데이터가 커도 잘 돌아감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutually exclusive : 상호 배타적인\n",
    "# stratified : 계층적인\n",
    "# notation : 표기 방법\n",
    "# convergence : 수렴\n",
    "# optimal : 최적의"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
