{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting problem -> Regularization\n",
    "\n",
    "# underfit(high bias)\n",
    "# overfit(high variance)\n",
    "\n",
    "# hθ(x) == learned hypothesis\n",
    "\n",
    "# 새로운 예시에 관해서 일반화하기 어렵다\n",
    "\n",
    "# linear regression, logistic regression 모두 문제가 발생한다\n",
    "\n",
    "# features가 많을 경우 plotting, visualization은 어려워짐\n",
    "# -> Reduce a number of featuress\n",
    "# 1. Manually select\n",
    "# 2. Model selection algorithm (features engineering) : 자동으로 features를 선택함\n",
    "# - 하지만 무엇이 중요한 features인지 모름\n",
    "\n",
    "# Regularization : Keep all the features, but reduce magnitude/values of parameters 𝜃j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization (L2 regularization)\n",
    "\n",
    "# 개념 : 불필요한 features의 𝜃값을 줄인다 (penality를 부여함)\n",
    "# 기능 : 불필요한 features의 𝜃값을 0에 근접하게 만드는 더욱 정확한 𝜃값을 찾게 해줌, 여전히 모든 features가 영향을 줌\n",
    "# “Simpler” hypothesis\n",
    "\n",
    "# 𝜃0은 기본 가격이기에 건드리지 않음\n",
    "# 𝜃1부터 𝜃n까지 전부 regularization 해줌\n",
    "# ƛ : Regularization parameter\n",
    "\n",
    "# p.22\n",
    "# J(𝜃) = fit하려는 것 + stop하려는 것\n",
    "# ƛ가 커지면 𝜃는 작아짐\n",
    "# Regularization은 overfit을 방지하기 위해 사용하기에 ƛ가 너무 크면 underfit 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p.27\n",
    "\n",
    "# Gradient descent with regularization for logistic regression\n",
    "# 𝜃j = (1-ɑ*ƛ/m)𝜃j + ɑ/m( mΣ1 h𝜃(x(i))-y(i) )xj(i)\n",
    "\n",
    "# ɑ*ƛ/m는 ƛ,ɑ,m 모두 양수이므로 1보다 작음\n",
    "# Regualrization 하면 기존의 θj 값 보다 더 작은 값을 뺌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p.5\n",
    "\n",
    "# Regularization이 default(L2)로 적용 되어있음\n",
    "\n",
    "# Solver : 모든 𝜃에 대해서가 아닌 랜덤으로 𝜃를 뽑아서 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답률 =  [0.8448 0.8402 0.8494 0.846  0.839 ]\n",
      "평균 정답률 =  0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics, model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "reviews_train = load_files(\"/Users/leedonghyeok/Downloads/aclImdb/train\")\n",
    "reviews_test = load_files(\"/Users/leedonghyeok/Downloads/aclImdb/test\")\n",
    "\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "\n",
    "text_train = [doc.replace(b\"<br />\", b\" \")  for doc in text_train]\n",
    "text_test = [doc.replace(b\"<br />\", b\" \")  for doc in text_test]\n",
    "\n",
    "vect = CountVectorizer(min_df = 5, max_df = 1000).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "X_test = vect.transform(text_test)\n",
    "\n",
    "clf = LogisticRegression(solver = 'sag', max_iter = 1000) # solver = 'sag' : 수업에서 배운 gradient descent 알고리즘\n",
    "clf.fit(X_train, y_train)\n",
    "pre = clf.predict(X_test)\n",
    "\n",
    "scores = model_selection.cross_val_score(clf, X_train, y_train, cv= 5)\n",
    "\n",
    "print(\"정답률 = \", scores)\n",
    "print(\"평균 정답률 = \", \"%.2f\"%scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
