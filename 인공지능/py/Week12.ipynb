{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impurity가 얼마나 개선됐는지 측정\n",
    "\n",
    "# 1. Information gain\n",
    "\n",
    "# p.17\n",
    "\n",
    "# = I(p,n) - E(age)\n",
    "# I(p,n) -> p : positive, n : negative\n",
    "# E(age) -> 5/14 I(2,3) + 4/14 I(4,0) + 5/14 I(3,2)\n",
    "# entropy가 가장 높은 attribute를 채택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Gini Index (p.18)\n",
    "\n",
    "# Gini(D) = 1-P(D1)^2-P(D2)^2...-P(Dn)^2\n",
    "# impurity degree가 낮으면 Gini Index 값이 낮게 나옴\n",
    "\n",
    "# the smallest gini split(D) (== the largest reduction in impurity) 채택\n",
    "# It needs to enumerate all the possible splitting points for each attribute (ex_구간 분할에 대한 모든 경우의 수 열거)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p.20 - Gini index를 사용한 Gini split의 예\n",
    "\n",
    "# income의 구간을 나누는 case : 1{l,mh}, 2{lm,h}, 3{lh,m}\n",
    "# Gini(D) = 0.459\n",
    "# case 1 = 0.450\n",
    "# case 2 = 0.443\n",
    "# case 3 = 0.458\n",
    "\n",
    "# 따라서 가장 낮은 case 2 채택\n",
    "\n",
    "# The problem of Gini split\n",
    "# split이 많아지면 많아질수록 Information gain 값은 커진다\n",
    "# equal-sized partitions에서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Gain Ratio - Information Gain 방식의 단점을 보완하기 위해 나옴\n",
    "\n",
    "# GainRatio = GAIN Split / SplitINFO\n",
    "# SplitINFO : split된 elements의 entropy -> partitions가 많으면 많을수록 페널티를 줌\n",
    "\n",
    "# Gain Ratio가 가장 큰 case를 채택\n",
    "# unbalance-sized partitions에서 사용\n",
    "\n",
    "# https://process-mining.tistory.com/106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting\n",
    "# Too many branches, some may reflect anomalies due to noise or outliers -> Poor accuracy for unseen samples\n",
    "\n",
    "# To avoid overfitting -> Pruning\n",
    "# prepruning : Halt tree construction early - threshold를 설정함\n",
    "# postpruning : Remove branches from a “fully grown” tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ Decision Tree Based Classification ]\n",
    "\n",
    "# 장점\n",
    "# 1. Inexpensive to construct\n",
    "# 2. Extremely fast\n",
    "# 3. Easy to interpret for small trees\n",
    "# 4. Robust to noise\n",
    "# 5. Can easily handle redundant or irrelevant attributes\n",
    "\n",
    "# 단점\n",
    "# 1. Space of possible decision trees is exponentially large\n",
    "# 2. Doesn't take into account interactions between atrributes\n",
    "# 3. Each decision boundary invovles only a single attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ Ensemble Methods ]\n",
    "\n",
    "# Predict class label of test records by combining the predictions made by multiple classifiers\n",
    "# Majority voting 이용\n",
    "\n",
    "# p.30\n",
    "# Multiple Classifiers : 여러 개의 base classifiers\n",
    "\n",
    "# Two necessary conditions\n",
    "# 1. The base classifiers should do \"better\" than a classifier that performs random guessing(50% accuracy)\n",
    "# 2. The base classifiers should be \"independent\" of each other - base classifiers are slightly correlated (문제없음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p.28\n",
    "# error rate = 1 - accuracy rate\n",
    "# red line : ensemble methods\n",
    "# blue line : base classifier\n",
    "\n",
    "# X는 base classifier에서 나온 결과 값\n",
    "# P(X>=13) : Wrong prediction\n",
    "# classifiers가 많아질수록 error rate가 떨어지는 것을 알 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types of Ensemble Methods\n",
    "# 1. Manipulate data distribution(==Sampling) ex_bagging(Sample with replacement), boosting\n",
    "# 2. Manipulate input features (choose subset of input features) ex_Random Forests(random vectors)\n",
    "# 3. Manipulate class labels\n",
    "# 4. Manipulate learning algorithm - base classifiers의 algorithm를 서로 다르게 구성\n",
    "\n",
    "# The base classifiers used in each ensemble method consist of 50 decision trees.\n",
    "# ten-fold cross-validation : 10개의 데이터 셋으로 나누어 교차검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ Random vector(attributes) generation ]\n",
    "\n",
    "# 1. Forest-RI(Random Input Selection)\n",
    "# Randomly select F input features in the decision tree\n",
    "# Make a decision to split a node from selected F features\n",
    "# Majority voting\n",
    "\n",
    "# 2. Trade off(균형) for F (number of features)\n",
    "# F = 1 + log2d  (d is the number of input features)\n",
    "# F가 작으면 classifiers(trees) are less correlated ex_'지역'만으로 컴퓨터 구매여부를 예측하는 것\n",
    "# F가 크면 stronger classifiers - 겹치는 것이 많기 때문에 strong (classifiers의 상관관계가 높아짐)\n",
    "\n",
    "# 3. Forest-RC(Random Combination) - features가 적을 경우\n",
    "# Create linear combination of input features ex_[a,b,c,ab,ac,bc,abc]\n",
    "\n",
    "# 4. 3rd approach\n",
    "# Select one of the best splits at each node of the decision tree\n",
    "\n",
    "# https://sooah26.tistory.com/m/70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest Classifier parameters\n",
    "# n_estimators : base classifiers(decision tree)의 갯수\n",
    "# criterion : dafault는 gini index\n",
    "# min_impurity_split : impurity 값이 threshold 이하면 decision tree 생성을 stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# file downloading\n",
    "import urllib.request as req\n",
    "local = \"mushroom.csv\"\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\"\n",
    "req.urlretrieve(url,local)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0  1_b  1_c  1_f  1_k  1_s  1_x  2_f  2_g  2_s  ...  21_s  21_v  21_y  \\\n",
      "0     p    0    0    0    0    0    1    0    0    1  ...     1     0     0   \n",
      "1     e    0    0    0    0    0    1    0    0    1  ...     0     0     0   \n",
      "2     e    1    0    0    0    0    0    0    0    1  ...     0     0     0   \n",
      "3     p    0    0    0    0    0    1    0    0    0  ...     1     0     0   \n",
      "4     e    0    0    0    0    0    1    0    0    1  ...     0     0     0   \n",
      "...  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   ...   \n",
      "8119  e    0    0    0    1    0    0    0    0    1  ...     0     0     0   \n",
      "8120  e    0    0    0    0    0    1    0    0    1  ...     0     1     0   \n",
      "8121  e    0    0    1    0    0    0    0    0    1  ...     0     0     0   \n",
      "8122  p    0    0    0    1    0    0    0    0    0  ...     0     1     0   \n",
      "8123  e    0    0    0    0    0    1    0    0    1  ...     0     0     0   \n",
      "\n",
      "      22_d  22_g  22_l  22_m  22_p  22_u  22_w  \n",
      "0        0     0     0     0     0     1     0  \n",
      "1        0     1     0     0     0     0     0  \n",
      "2        0     0     0     1     0     0     0  \n",
      "3        0     0     0     0     0     1     0  \n",
      "4        0     1     0     0     0     0     0  \n",
      "...    ...   ...   ...   ...   ...   ...   ...  \n",
      "8119     0     0     1     0     0     0     0  \n",
      "8120     0     0     1     0     0     0     0  \n",
      "8121     0     0     1     0     0     0     0  \n",
      "8122     0     0     1     0     0     0     0  \n",
      "8123     0     0     1     0     0     0     0  \n",
      "\n",
      "[8124 rows x 118 columns]\n",
      "     label pre\n",
      "1525     e   e\n",
      "3109     p   p\n",
      "2915     e   e\n",
      "8043     e   e\n",
      "6476     p   p\n",
      "5912     p   p\n",
      "5755     p   p\n",
      "2031     e   e\n",
      "3378     e   e\n",
      "740      e   e\n",
      "정답률 = 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "mr = pd.read_csv(\"/Volumes/GoogleDrive-107262488266475120044/내 드라이브/3-1/인공지능/py/mushroom.csv\", header=None)\n",
    "\n",
    "# label 분리\n",
    "df = pd.DataFrame(mr.iloc[:,0])\n",
    "\n",
    "# 두 번째 column부터 마지막 column까지 one-hot encoding하고 label에 붙임\n",
    "df = df.join(pd.get_dummies(mr.iloc[:,1:]))\n",
    "print(df)\n",
    "\n",
    "data = df.iloc[:,1:]\n",
    "label = df.iloc[:,0]\n",
    "\n",
    "data_train, data_test, label_train, label_test = train_test_split(data,label)\n",
    "\n",
    "# 데이터 학습시키기\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(data_train, label_train)\n",
    "\n",
    "# 데이터 예측하기\n",
    "predict = clf.predict(data_test)\n",
    "\n",
    "# 결과 테스트하기\n",
    "result = pd.DataFrame({\"label\": label_test, \"pre\": predict})\n",
    "print(result[0:10])\n",
    "\n",
    "ac_score = metrics.accuracy_score(label_test, predict)\n",
    "print(\"정답률 =\", ac_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods for estimating a classifier’s accuracy\n",
    "# 1. Holdout method, random subsampling\n",
    "# 2. Cross-validation\n",
    "# 3. Bootstrap\n",
    "\n",
    "# Confusion Matrix - p.5\n",
    "# Accuracy = (TP + TN)/All\n",
    "# Error rate = (FP + FN)/All\n",
    "# Sensitivity = TP/P\n",
    "# Specificity = TN/N\n",
    "\n",
    "# p.7\n",
    "# precision = TP / TP+FP 중요 - positive가 보통 의미있는 경우라 가정하기 때문이다\n",
    "# recall(==Sensitivity) = TP / TP+FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerate : 열거하다\n",
    "# robust : 튼튼한, 강력한\n",
    "# redundant : 중복된"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
