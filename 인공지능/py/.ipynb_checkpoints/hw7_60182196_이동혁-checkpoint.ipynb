{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "\n",
    "reviews_train = load_files(\"/Users/leedonghyeok/Downloads/aclImdb/train\")\n",
    "reviews_test = load_files(\"/Users/leedonghyeok/Downloads/aclImdb/test\")\n",
    "\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "text_train = [doc.replace(b\"<br />\", b\" \")  for doc in text_train]\n",
    "text_test = [doc.replace(b\"<br />\", b\" \")  for doc in text_test]\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "# Having an intelligent interesting script doesn't hurt either.<br /><br />Stargate SG1 is currently one of my favorite programs.\n",
    "# Having an intelligent interesting script doesn't hurt either.  Stargate SG1 is currently one of my favorite programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "vect = CountVectorizer(stop_words = 'english').fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "X_test = vect.transform(text_test)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답률 = 0.6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train.toarray(),y_train)\n",
    "pre = model.predict(X_test.toarray())\n",
    "\n",
    "ac_score = accuracy_score(y_test,pre)\n",
    "print(\"정답률 = {:0,.1f}\".format(ac_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_train = pd.read_csv(\"/Users/leedonghyeok/Downloads/ratings_train.txt\", delimiter = '\\t', keep_default_na = False)\n",
    "df_test = pd.read_csv(\"/Users/leedonghyeok/Downloads/ratings_test.txt\", delimiter = '\\t', keep_default_na = False)\n",
    "\n",
    "text_train = df_train['document']\n",
    "y_train = df_train['label']\n",
    "\n",
    "text_test = df_test['document']\n",
    "y_test = df_test['label']\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['흠', '포스터', '보고', '초딩', '영화', '줄', '오버', '연기', '가볍다', '않다']\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sample = text_train[1]\n",
    "\n",
    "print(sample)\n",
    "\n",
    "import konlpy\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "twitter_tag = Okt()\n",
    "\n",
    "def twitter_tokenizer(text):\n",
    "    malist = twitter_tag.pos(text, norm = True, stem = True)\n",
    "    r = []\n",
    "    for word in malist:\n",
    "        if not word[1] in ['Josa','Eomi','Punctuation','KoreanParticle']:\n",
    "            r.append(word[0])\n",
    "    return r\n",
    "\n",
    "vect = CountVectorizer(tokenizer = twitter_tokenizer).fit(text_train)\n",
    "\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "print(twitter_tokenizer(sample))\n",
    "print(\"done\")\n",
    "\n",
    "# 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
    "# '흠', '포스터', '보고', '초딩', '영화', '줄', '오버', '연기', '가볍다', '않다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 248358)\t0.6303222409610997\n",
      "  (0, 246232)\t0.26458844458802766\n",
      "  (0, 99567)\t0.5193460454404283\n",
      "  (0, 71119)\t0.5128026059076282\n",
      "  (1, 273335)\t0.39339783492704455\n",
      "  (1, 255126)\t0.48708202211988\n",
      "  (1, 190112)\t0.48708202211988\n",
      "  (1, 167602)\t0.4594654475140474\n",
      "  (1, 16352)\t0.39953955182265427\n",
      "  (2, 57394)\t1.0 \n",
      "\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect2 = TfidfVectorizer().fit(text_train)\n",
    "X_train = vect2.transform(text_train)\n",
    "X_test = vect2.transform(text_test)\n",
    "\n",
    "print(X_train[:3],\"\\n\")\n",
    "print(X_train[:3].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답률 = 0.8\n",
      "0.8276\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "nb1 = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "pre = nb1.predict(X_test)\n",
    "\n",
    "ac_score1 = metrics.accuracy_score(y_test,pre)\n",
    "print(\"정답률 = {:0,.1f}\".format(ac_score1))\n",
    "print(ac_score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답률 = 0.8276 , alpha = 0.6\n"
     ]
    }
   ],
   "source": [
    "nb2 = MultinomialNB(alpha = 0.6).fit(X_train, y_train)\n",
    "\n",
    "pre2 = nb2.predict(X_test)\n",
    "\n",
    "ac_score2 = metrics.accuracy_score(y_test,pre2)\n",
    "print(\"정답률 = {}\".format(ac_score2), \", alpha = {}\".format(0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
