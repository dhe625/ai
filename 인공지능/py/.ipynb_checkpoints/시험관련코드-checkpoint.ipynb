{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term:['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]] \n",
      "\n",
      "[[0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]\n",
      " [0.         0.27230147 0.         0.27230147 0.         0.85322574\n",
      "  0.22262429 0.         0.27230147]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?']\n",
    "\n",
    "vect1 = CountVectorizer().fit(corpus)\n",
    "cvtf = vect1.transform(corpus)\n",
    "\n",
    "feature_names = vect1.get_feature_names()\n",
    "print(\"Term:{}\".format(feature_names[:]))\n",
    "print(cvtf.toarray(), \"\\n\")\n",
    "\n",
    "vect2 = TfidfVectorizer().fit(corpus)\n",
    "tvtf = vect2.transform(corpus)\n",
    "print(tvtf.toarray(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = [\"python\", \"pythoner\", \"pythoning\", \"pythoned\", \"pythonly\"]\n",
    "for s in example_words:\n",
    "    print(ps.stem(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'like', 'swim', 'swimmer']\n",
      "[[1 1 1 1]]\n",
      "[[2 0 1 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "# CountVectorizer에 토큰을 생성하는 별도의 함수(tokenize)를 지정함\n",
    "# 함수 내부에서 토큰화와 stemming을 진행\n",
    "vect = CountVectorizer(tokenizer = tokenize, stop_words = 'english') # tokenizer가 token_pattern보다 우선순위가 높은 것으로 추정\n",
    "vect.fit([\"The swimmer likes swimming.\"])\n",
    "\n",
    "sentence1 = vect.transform([\"The swimmer likes swimming.\"])\n",
    "sentence2 = vect.transform([\"The swimmer swim. .\"])\n",
    "# sentence1과 2는 sparse matrix\n",
    "\n",
    "print(vect.get_feature_names()) # 어휘사전에 있는 어휘를 리스트 형태로 반환\n",
    "print(sentence1.toarray())\n",
    "print(sentence2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label pre\n",
      "5521     p   p\n",
      "2640     e   e\n",
      "6688     p   p\n",
      "5578     e   e\n",
      "245      e   e\n",
      "5445     p   p\n",
      "5414     p   p\n",
      "4319     p   p\n",
      "692      e   e\n",
      "2234     e   e\n",
      "정답률 = 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "mr = pd.read_csv(\"/Volumes/GoogleDrive-107262488266475120044/내 드라이브/3-1/인공지능/py/mushroom.csv\", header=None)\n",
    "\n",
    "df = pd.DataFrame(mr.iloc[:,0])\n",
    "\n",
    "df = df.join(pd.get_dummies(mr.iloc[:,1:]))\n",
    "\n",
    "data = df.iloc[:,1:]\n",
    "label = df.iloc[:,0]\n",
    "\n",
    "data_train, data_test, label_train, label_test = train_test_split(data,label)\n",
    "\n",
    "clf = RandomForestClassifier(criterion='entropy')\n",
    "clf.fit(data_train, label_train)\n",
    "\n",
    "predict = clf.predict(data_test)\n",
    "\n",
    "result = pd.DataFrame({\"label\": label_test, \"pre\": predict})\n",
    "print(result[0:10])\n",
    "\n",
    "ac_score = metrics.accuracy_score(label_test, predict)\n",
    "print(\"정답률 =\", ac_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답률 =  0.87944\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "\n",
    "reviews_train = load_files(\"/Users/leedonghyeok/Downloads/aclImdb/train\")\n",
    "reviews_test = load_files(\"/Users/leedonghyeok/Downloads/aclImdb/test\")\n",
    "\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "\n",
    "text_train = [doc.replace(b\"<br />\", b\" \")  for doc in text_train]\n",
    "text_test = [doc.replace(b\"<br />\", b\" \")  for doc in text_test]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "vect = TfidfVectorizer(tokenizer = tokenize, stop_words = 'english').fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "X_test = vect.transform(text_test)\n",
    "\n",
    "# Logistic regression으로 예측\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "pre = clf.predict(X_test)\n",
    "\n",
    "ac_score = accuracy_score(y_test, pre)\n",
    "print(\"정답률 = \", ac_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답률 =  0.83024\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import load_files\n",
    "import numpy as np\n",
    "\n",
    "reviews_train = load_files(\"/Users/leedonghyeok/Downloads/aclImdb/train\")\n",
    "reviews_test = load_files(\"/Users/leedonghyeok/Downloads/aclImdb/test\")\n",
    "\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "\n",
    "text_train = [doc.replace(b\"<br />\", b\" \")  for doc in text_train]\n",
    "text_test = [doc.replace(b\"<br />\", b\" \")  for doc in text_test]\n",
    "\n",
    "vect = TfidfVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "X_test = vect.transform(text_test)\n",
    "\n",
    "# Multinomial Naïve bayesian으로 예측\n",
    "nb = MultinomialNB(alpha = 1)\n",
    "nb.fit(X_train, y_train)\n",
    "pre = nb.predict(X_test)\n",
    "\n",
    "ac_score = metrics.accuracy_score(y_test, pre)\n",
    "print(\"정답률 = \", ac_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답률 =  0.8402\n"
     ]
    }
   ],
   "source": [
    "import konlpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "twitter_tag = Okt()\n",
    "\n",
    "def twitter_tokenizer(text):\n",
    "    malist = twitter_tag.pos(text, norm = True, stem = True)\n",
    "    r = []\n",
    "    for word in malist:\n",
    "        if not word[1] in [\"Josa\", \"Eomi\", \"Punctuation\", \"KoreanParticle\"]:\n",
    "            r.append(word[0])\n",
    "    return r\n",
    "\n",
    "df_train = pd.read_csv(\"/Users/leedonghyeok/Downloads/ratings_train.txt\", delimiter = '\\t', keep_default_na = False)\n",
    "df_test = pd.read_csv(\"/Users/leedonghyeok/Downloads/ratings_test.txt\", delimiter = '\\t', keep_default_na = False)\n",
    "\n",
    "text_train = df_train['document']\n",
    "y_train = df_train['label']\n",
    "\n",
    "text_test = df_test['document']\n",
    "y_test = df_test['label']\n",
    "\n",
    "vect = CountVectorizer(tokenizer = twitter_tokenizer).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "X_test = vect.transform(text_test)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "pre = clf.predict(X_test)\n",
    "\n",
    "ac_score = metrics.accuracy_score(y_test, pre)\n",
    "print(\"정답률 = \", ac_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time(seconds) 2.41\n",
      "각 validation 정답률 =  [0.68511685 1.         1.         1.         1.         1.\n",
      " 1.         1.         0.9679803  1.        ]\n",
      "평균 정답률 : 0.97\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, model_selection\n",
    "from time import time\n",
    "\n",
    "mr = pd.read_csv(\"/Volumes/GoogleDrive-107262488266475120044/내 드라이브/3-1/인공지능/py/mushroom.csv\", header = None)\n",
    "\n",
    "df = pd.DataFrame(mr.iloc[:, 0])\n",
    "df = df.join(pd.get_dummies(mr.iloc[:, 1: ]))\n",
    "\n",
    "data = df.iloc[:, 1:]\n",
    "label = df.loc[:, 0]\n",
    "\n",
    "start = time()\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators = 100)\n",
    "clf.fit(data, label)\n",
    "\n",
    "scores = model_selection.cross_val_score(clf, data, label, cv = 10)\n",
    "\n",
    "end = time()\n",
    "\n",
    "print(\"Execution time(seconds)\", str(round((end-start), 2)))\n",
    "print(\"각 validation 정답률 = \", scores)\n",
    "print(\"평균 정답률 :\", \"%.2f\"%scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정답률 =  0.81384\n",
      "정답률 =  [0.8448 0.8404 0.8496 0.846  0.839 ]\n",
      "평균 정답률 =  0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics, model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "reviews_train = load_files(\"/Users/leedonghyeok/Downloads/aclImdb/train\")\n",
    "reviews_test = load_files(\"/Users/leedonghyeok/Downloads/aclImdb/test\")\n",
    "\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "\n",
    "text_train = [doc.replace(b\"<br />\", b\" \")  for doc in text_train]\n",
    "text_test = [doc.replace(b\"<br />\", b\" \")  for doc in text_test]\n",
    "\n",
    "vect = CountVectorizer(min_df = 5, max_df = 1000).fit(text_train) # Dimension reduction\n",
    "X_train = vect.transform(text_train)\n",
    "X_test = vect.transform(text_test)\n",
    "\n",
    "clf = LogisticRegression(solver = 'sag', max_iter = 1000) # solver = 'sag' : 수업에서 배운 gradient descent 알고리즘\n",
    "clf.fit(X_train, y_train)\n",
    "pre = clf.predict(X_test)\n",
    "\n",
    "scores = model_selection.cross_val_score(clf, X_train, y_train, cv= 5)\n",
    "ac_score = accuracy_score(y_test, pre)\n",
    "\n",
    "print(\"예측 정답률 = \", ac_score)\n",
    "print(\"정답률 = \", scores)\n",
    "print(\"평균 정답률 = \", \"%.2f\"%scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
