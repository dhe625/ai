{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting problem -> Regularization\n",
    "\n",
    "# underfit(high bias)\n",
    "# overfit(high variance)\n",
    "\n",
    "# hÎ¸(x) == learned hypothesis\n",
    "\n",
    "# ìƒˆë¡œìš´ ì˜ˆì‹œì— ê´€í•´ì„œ ì¼ë°˜í™”í•˜ê¸° ì–´ë µë‹¤\n",
    "\n",
    "# linear regression, logistic regression ëª¨ë‘ ë¬¸ì œê°€ ë°œìƒí•œë‹¤\n",
    "\n",
    "# featuresê°€ ë§ì„ ê²½ìš° plotting, visualizationì€ ì–´ë ¤ì›Œì§\n",
    "# -> Reduce a number of featuress\n",
    "# 1. Manually select\n",
    "# 2. Model selection algorithm (features engineering) : ìë™ìœ¼ë¡œ featuresë¥¼ ì„ íƒí•¨\n",
    "# - í•˜ì§€ë§Œ ë¬´ì—‡ì´ ì¤‘ìš”í•œ featuresì¸ì§€ ëª¨ë¦„\n",
    "\n",
    "# Regularization : Keep all the features, but reduce magnitude/values of parameters ğœƒj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization (L2 regularization)\n",
    "\n",
    "# ê°œë… : ë¶ˆí•„ìš”í•œ featuresì˜ ğœƒê°’ì„ ì¤„ì¸ë‹¤ (penalityë¥¼ ë¶€ì—¬í•¨)\n",
    "# ê¸°ëŠ¥ : ë¶ˆí•„ìš”í•œ featuresì˜ ğœƒê°’ì„ 0ì— ê·¼ì ‘í•˜ê²Œ ë§Œë“œëŠ” ë”ìš± ì •í™•í•œ ğœƒê°’ì„ ì°¾ê²Œ í•´ì¤Œ, ì—¬ì „íˆ ëª¨ë“  featuresê°€ ì˜í–¥ì„ ì¤Œ\n",
    "# â€œSimplerâ€ hypothesis\n",
    "\n",
    "# ğœƒ0ì€ ê¸°ë³¸ ê°€ê²©ì´ê¸°ì— ê±´ë“œë¦¬ì§€ ì•ŠìŒ\n",
    "# ğœƒ1ë¶€í„° ğœƒnê¹Œì§€ ì „ë¶€ regularization í•´ì¤Œ\n",
    "# Æ› : Regularization parameter\n",
    "\n",
    "# p.22\n",
    "# J(ğœƒ) = fití•˜ë ¤ëŠ” ê²ƒ + stopí•˜ë ¤ëŠ” ê²ƒ\n",
    "# Æ›ê°€ ì»¤ì§€ë©´ ğœƒëŠ” ì‘ì•„ì§\n",
    "# Regularizationì€ overfitì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ê¸°ì— Æ›ê°€ ë„ˆë¬´ í¬ë©´ underfit ë°œìƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p.27\n",
    "\n",
    "# Gradient descent with regularization for logistic regression\n",
    "# ğœƒj = (1-É‘*Æ›/m)ğœƒj + É‘/m( mÎ£1 hğœƒ(x(i))-y(i) )xj(i)\n",
    "\n",
    "# É‘*Æ›/mëŠ” Æ›,É‘,m ëª¨ë‘ ì–‘ìˆ˜ì´ë¯€ë¡œ 1ë³´ë‹¤ ì‘ìŒ\n",
    "# Regualrization í•˜ë©´ ê¸°ì¡´ì˜ Î¸j ê°’ ë³´ë‹¤ ë” ì‘ì€ ê°’ì„ ëºŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p.5\n",
    "\n",
    "# Regularizationì´ default(L2)ë¡œ ì ìš© ë˜ì–´ìˆìŒ\n",
    "\n",
    "# Solver : ëª¨ë“  ğœƒì— ëŒ€í•´ì„œê°€ ì•„ë‹Œ ëœë¤ìœ¼ë¡œ ğœƒë¥¼ ë½‘ì•„ì„œ ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì •ë‹µë¥  =  [0.8448 0.8402 0.8494 0.846  0.839 ]\n",
      "í‰ê·  ì •ë‹µë¥  =  0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics, model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "reviews_train = load_files(\"/Users/leedonghyeok/Downloads/aclImdb/train\")\n",
    "reviews_test = load_files(\"/Users/leedonghyeok/Downloads/aclImdb/test\")\n",
    "\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "\n",
    "text_train = [doc.replace(b\"<br />\", b\" \")  for doc in text_train]\n",
    "text_test = [doc.replace(b\"<br />\", b\" \")  for doc in text_test]\n",
    "\n",
    "vect = CountVectorizer(min_df = 5, max_df = 1000).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "X_test = vect.transform(text_test)\n",
    "\n",
    "clf = LogisticRegression(solver = 'sag', max_iter = 1000) # solver = 'sag' : ìˆ˜ì—…ì—ì„œ ë°°ìš´ gradient descent ì•Œê³ ë¦¬ì¦˜\n",
    "clf.fit(X_train, y_train)\n",
    "pre = clf.predict(X_test)\n",
    "\n",
    "scores = model_selection.cross_val_score(clf, X_train, y_train, cv= 5)\n",
    "\n",
    "print(\"ì •ë‹µë¥  = \", scores)\n",
    "print(\"í‰ê·  ì •ë‹µë¥  = \", \"%.2f\"%scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
