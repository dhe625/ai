{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse relationship between precision & recall\n",
    "\n",
    "# F measure : harmonic mean of precision & recall\n",
    "# F = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "# F measure ê³µì‹ì˜ ê³¼ì •\n",
    "# Geometric mean = âˆšx1*x2\n",
    "# Harmony = G^2 / Arithmetic mean\n",
    "# H = x1 * x2 / ((x1+x2)/2)\n",
    "\n",
    "# p.8 ì˜ˆì œ ì¤‘ìš”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold Cross-validation : Randomly partition the data into k mutually exclusive subsets, each approximately equal size\n",
    "# - Leave-one-out : k folds where k = # of tuples, for small sized data\n",
    "\n",
    "# Holdout Method : data setì—ì„œ ì„ì˜ë¡œ test setì„ ì§€ì •\n",
    "# - Given data is randomly partitioned into two independent sets\n",
    "\n",
    "    # Holdout Cross Validation (Random subsampling)\n",
    "    # - Repeat holdout k times\n",
    "    # partitionì„ ë‚˜ëˆ„ëŠ” ê¸°ì¤€ì„ randomí•˜ê²Œ í•¨\n",
    "\n",
    "# Stratified cross-validation - dataê°€ í¸í–¥ë˜ì–´ ìˆì„ ê²½ìš°\n",
    "# foldë¥¼ ì¼ì •í•˜ê²Œ ìœ ì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time(seconds) 2.39\n",
      "ê° validation ì •ë‹µë¥  =  [0.68511685 1.         1.         1.         1.         1.\n",
      " 1.         1.         0.99630542 1.        ]\n",
      "í‰ê·  ì •ë‹µë¥  : 0.97\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, model_selection\n",
    "from time import time\n",
    "\n",
    "mr = pd.read_csv(\"/Volumes/GoogleDrive-107262488266475120044/ë‚´ ë“œë¼ì´ë¸Œ/3-1/ì¸ê³µì§€ëŠ¥/py/mushroom.csv\", header = None)\n",
    "\n",
    "df = pd.DataFrame(mr.iloc[:, 0])\n",
    "df = df.join(pd.get_dummies(mr.iloc[:, 1: ]))\n",
    "\n",
    "data = df.iloc[:, 1:]\n",
    "label = df.loc[:, 0]\n",
    "\n",
    "start = time()\n",
    "\n",
    "# random vectors == base classifiers(decision trees)\n",
    "clf = RandomForestClassifier(n_estimators = 100)\n",
    "clf.fit(data, label)\n",
    "\n",
    "scores = model_selection.cross_val_score(clf, data, label, cv = 10)\n",
    "\n",
    "end = time()\n",
    "\n",
    "print(\"Execution time(seconds)\", str(round((end-start), 2)))\n",
    "print(\"ê° validation ì •ë‹µë¥  = \", scores)\n",
    "print(\"í‰ê·  ì •ë‹µë¥  :\", \"%.2f\"%scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "# - Supervised learning\n",
    "# - predict results within a \"continuous\" output\n",
    "# - map input variables to some continuous function\n",
    "\n",
    "# hğœƒ(ğ‘¥) = ğœƒ0 + ğœƒ1ğ‘¥\n",
    "# hğœƒ(ğ‘¥) is hypothesis\n",
    "# ğœƒ is parameters\n",
    "\n",
    "# Univariate linear regression (ë‹¨ë³€ëŸ‰ ì„ í˜•íšŒê·€) : Linear regression with one variable\n",
    "\n",
    "# Cost function : ì˜¤ì°¨ë¥¼ ì¸¡ì •í•˜ì—¬ ê°€ì¥ ì‘ì€ ê°’ ì±„íƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p.11\n",
    "\n",
    "# Squared error function - ê°€ì¥ ëŒ€í‘œì ì¸ cost functionì˜ ì˜ˆ\n",
    "# ğ½(ğœƒ0,ğœƒ1) = Î£(h(x)-y)^2 / 2m (ì˜ˆì¸¡ ê°’ hğœƒ(ğ‘¥)ì™€ ì‹¤ì œ ê°’ yê°„ì˜ ê°„ê²©ì´ ê°€ì¥ ì‘ì€ ğœƒê°’ì„ ì±„íƒí•˜ê¸° ìœ„í•¨)\n",
    "# ğ½(ğœƒ0,ğœƒ1)ì´ ê°€ì¥ ì‘ì€ ê°’ì„ ì±„íƒ\n",
    "\n",
    "# [ Gradient Descent algorithm ] : An algorithm for minimizing the cost function J for the linear regression\n",
    "# - iterative algorithm\n",
    "# - randomí•œ ì´ˆê¸°ê°’ì—ì„œ ì£¼ë³€ì„ ë‘˜ëŸ¬ë³´ë©´ì„œ ë” ì‘ì€ ê°’ì„ ì°¾ì•„ê°€ë©° minimum íƒìƒ‰\n",
    "# - ì´ˆê¸°ê°’ì´ randomí•˜ë¯€ë¡œ local minimumì„ ì°¾ì•„ë‚´ê³ , global minimumì€ ì°¾ì§€ ëª»í•  ìˆ˜ ìˆìŒ\n",
    "\n",
    "# p.14\n",
    "# := : í• ë‹¹ ì—°ì‚°ì\n",
    "# âº : learning rate (í•™ìŠµí•˜ëŠ” ë¹„ìœ¨)\n",
    "# ğœƒj := ğœƒj - âº * slope(ê¸°ìš¸ê¸°)\n",
    "# Simultaneously update ğœƒ0 and ğœƒ1\n",
    "\n",
    "# p.16 - bowl shape\n",
    "# ë°˜ë³µí•˜ë©´ minimum ìª½ìœ¼ë¡œ ìˆ˜ë ´í•˜ê²Œë¨\n",
    "\n",
    "# Problems\n",
    "# 1. âºê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ë”ë””ê²Œ ë‚´ë ¤ê°€ê³ , âºê°€ ë„ˆë¬´ í¬ë©´ ì§€ë‚˜ì¹œë‹¤\n",
    "# 2. Local minimum problem - ğœƒj is stucked at Local minimum\n",
    "#    - Squared error functionì€ bowl shapeìœ¼ë¡œ ë§Œë“¤ì–´ì¤Œ -> local minimum problem x\n",
    "# âºë¥¼ fixed í•˜ë”ë¼ë„ slopeê°€ ë°˜ë³µí• ìˆ˜ë¡ ì ì  ì‘ì•„ì§€ë¯€ë¡œ divergeí•˜ì§€ ì•ŠìŒ\n",
    "\n",
    "# p.22 ê³µì‹ ì¤‘ìš”\n",
    "# Batch : Each step of gradient descent uses all the training examples.\n",
    "\n",
    "# Convex function\n",
    "# - Cost function for linear regression\n",
    "# - ì•„ë˜ì—ì„œ ë´¤ì„ ë•Œ ì›€í‘¹ íŒŒì¸ ê³³ì´ ì—†ìŒ\n",
    "# - global minimumë§Œ ì¡´ì¬\n",
    "\n",
    "# Gradient DescentëŠ” scalabilityê°€ ì¢‹ìŒ -> ë°ì´í„°ê°€ ì»¤ë„ ì˜ ëŒì•„ê°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutually exclusive : ìƒí˜¸ ë°°íƒ€ì ì¸\n",
    "# stratified : ê³„ì¸µì ì¸\n",
    "# notation : í‘œê¸° ë°©ë²•\n",
    "# convergence : ìˆ˜ë ´\n",
    "# optimal : ìµœì ì˜"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
