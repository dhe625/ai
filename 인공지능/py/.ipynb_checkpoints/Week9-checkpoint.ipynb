{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency - Inverse Document Frequency\n",
    "# W = TF(íŠ¹ì • ë‹¨ì–´ê°€ ë¬¸ì„œ ë‚´ì— ë“±ì¥í•˜ëŠ” ë¹ˆë„ ìˆ˜) * log(N/Df)\n",
    "# logë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ  : N/Dfê°€ êµ‰ì¥íˆ í° ê°’ì¸ë° ê°€ì¤‘ì¹˜ì´ê¸°ì— logë¡œ í¬ê¸°ë¥¼ ì¤„ì—¬ ì‚¬ìš©\n",
    "\n",
    "# ìƒëŒ€ì ìœ¼ë¡œ ëœ ë“±ì¥í•˜ëŠ” ê°’ì´ ì¤‘ìš”í•  ê°€ëŠ¥ì„±ì´ ë†’ê¸° ë•Œë¬¸ì— inverseí•¨\n",
    "# Rank-Frequency Law (Zipfâ€™s Law) : Frequency(ë¹ˆë„ ìˆ˜) * Rank(ì¤‘ìš”ë„) = Constant\n",
    "# ì •ë³´ê²€ìƒ‰ ì‹œìŠ¤í…œ(Information Retrieval System) : ì§ˆì˜ì–´ì™€ ê´€ë ¨ìˆëŠ” documentì˜ ìƒëŒ€ì ì¸ ë¶€í•© ì •ë„ë¥¼ ë°˜í™˜\n",
    "# Rank â‰… ğ›‚ğ‘»ğ‘­ âˆ— ğœ·[ğ‘³ğ’ğ’ˆğœ¸ğ‘µ âˆ’ ğ‘³ğ’ğ’ˆğœ¸ğ‘«ğ‘­ + ğŸ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term:['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]] \n",
      "\n",
      "[[0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]\n",
      " [0.         0.27230147 0.         0.27230147 0.         0.85322574\n",
      "  0.22262429 0.         0.27230147]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?']\n",
    "\n",
    "vect1 = CountVectorizer().fit(corpus)\n",
    "tf = vect1.transform(corpus)\n",
    "\n",
    "feature_names = vect1.get_feature_names() # ì–´íœ˜ì‚¬ì „ì— ìˆëŠ” ì–´íœ˜ë¥¼ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜\n",
    "print(\"Term:{}\".format(feature_names[:]))\n",
    "print(tf.toarray(), \"\\n\")\n",
    "\n",
    "vect2 = TfidfVectorizer().fit(corpus)\n",
    "tfidf = vect2.transform(corpus)\n",
    "print(tfidf.toarray()) # ê°€ì¤‘ì¹˜ë¡œ ì¸í•´ ì‹¤ìˆ˜ë¡œ ë‚˜íƒ€ë‚¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-gram ë°©ì‹\n",
    "# gram : unit -> ì–´íœ˜ì‚¬ì „ unit(term)ì˜ ê°¯ìˆ˜\n",
    "# ngram_range(1,1) : min = 1, max = 1\n",
    "# unitì´ termì¼ í•„ìš”ëŠ” ì—†ìŒ ex) char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì–´íœ˜ì‚¬ì „ì˜ í¬ê¸°: 13\n",
      "ì–´íœ˜ì‚¬ì „:\n",
      "['be', 'but', 'doth', 'fool', 'he', 'himself', 'is', 'knows', 'man', 'the', 'think', 'to', 'wise']\n",
      "ë³€í™˜ëœ ë°ì´í„°:\n",
      "[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bards_words = [\"The fool doth think he is wise,\",\n",
    "              \"but the wise man knows himself to be a fool\"]\n",
    "\n",
    "cv = CountVectorizer(ngram_range = (1,1)).fit(bards_words)\n",
    "print(\"ì–´íœ˜ì‚¬ì „ì˜ í¬ê¸°: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"ì–´íœ˜ì‚¬ì „:\\n{}\".format(cv.get_feature_names()))\n",
    "print(\"ë³€í™˜ëœ ë°ì´í„°:\\n{}\".format(cv.transform(bards_words).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì–´íœ˜ì‚¬ì „ì˜ í¬ê¸°: 39\n",
      "ì–´íœ˜ì‚¬ì „:\n",
      "['be', 'be fool', 'but', 'but the', 'but the wise', 'doth', 'doth think', 'doth think he', 'fool', 'fool doth', 'fool doth think', 'he', 'he is', 'he is wise', 'himself', 'himself to', 'himself to be', 'is', 'is wise', 'knows', 'knows himself', 'knows himself to', 'man', 'man knows', 'man knows himself', 'the', 'the fool', 'the fool doth', 'the wise', 'the wise man', 'think', 'think he', 'think he is', 'to', 'to be', 'to be fool', 'wise', 'wise man', 'wise man knows']\n",
      "ë³€í™˜ëœ ë°ì´í„°:\n",
      "[[0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 0\n",
      "  1 0 0]\n",
      " [1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1\n",
      "  1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bards_words = [\"The fool doth think he is wise,\",\n",
    "              \"but the wise man knows himself to be a fool\"]\n",
    "\n",
    "cv = CountVectorizer(ngram_range = (1,3)).fit(bards_words)\n",
    "print(\"ì–´íœ˜ì‚¬ì „ì˜ í¬ê¸°: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"ì–´íœ˜ì‚¬ì „:\\n{}\".format(cv.get_feature_names()))\n",
    "print(\"ë³€í™˜ëœ ë°ì´í„°:\\n{}\".format(cv.transform(bards_words).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant : ë¶€í•©í•˜ëŠ”, ê´€ë ¨ìˆëŠ”\n",
    "# append : ì²¨ë¶€í•˜ë‹¤\n",
    "# substitue : ëŒ€ì²´í•˜ë‹¤\n",
    "# accuracy : ì •í™•ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer() # stemmer ê°ì²´ ìƒì„±\n",
    "\n",
    "example_words = [\"python\", \"pythoner\", \"pythoning\", \"pythoned\", \"pythonly\"]\n",
    "for s in example_words:\n",
    "    print(ps.stem(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'is', 'important', 'to', 'be', 'very', 'pythonly', 'while', 'you', 'are', 'pythoning', 'with', 'python']\n",
      "\n",
      "\n",
      "It\n",
      "is\n",
      "import\n",
      "to\n",
      "be\n",
      "veri\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "python\n",
      "\n",
      "\n",
      "['It', 'is', 'import', 'to', 'be', 'veri', 'pythonli', 'while', 'you', 'are', 'python', 'with', 'python']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "new_text = \"It is important to be very pythonly while you are pythoning with python\"\n",
    "words = word_tokenize(new_text) # í† í°í™”(ì°¢ê¸°)\n",
    "print(words)\n",
    "print(\"\\n\")\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w)) # ì–´ê°„ ì¶”ì¶œ(stemming)\n",
    "print(\"\\n\")\n",
    "\n",
    "# í™”ë©´ì— ì¶œë ¥í•˜ì§€ ì•Šê³  term ë¦¬ìŠ¤íŠ¸ë¡œ ìœ ì§€\n",
    "result = [ps.stem(w) for w in words]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'like', 'swim', 'swimmer']\n",
      "[[1 1 1 1]]\n",
      "[[2 0 1 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "# CountVectorizerì— í† í°ì„ ìƒì„±í•˜ëŠ” ë³„ë„ì˜ í•¨ìˆ˜(tokenize)ë¥¼ ì§€ì •í•¨\n",
    "# í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ í† í°í™”ì™€ stemmingì„ ì§„í–‰\n",
    "vect = CountVectorizer(tokenizer = tokenize, stop_words = 'english') # tokenizerê°€ token_patternë³´ë‹¤ ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ê²ƒìœ¼ë¡œ ì¶”ì •\n",
    "vect.fit([\"The swimmer likes swimming.\"])\n",
    "\n",
    "sentence1 = vect.transform([\"The swimmer likes swimming.\"])\n",
    "sentence2 = vect.transform([\"The swimmer swim. .\"])\n",
    "\n",
    "print(vect.get_feature_names()) # ì–´íœ˜ì‚¬ì „ì— ìˆëŠ” ì–´íœ˜ë¥¼ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜\n",
    "\n",
    "# sentence1ê³¼ 2ëŠ” sparse matrix\n",
    "print(sentence1.toarray())\n",
    "print(sentence2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ìˆ˜: 25000\n",
      "í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ (í…ŒìŠ¤íŠ¸ ë°ì´í„°): [12500 12500]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì •ë‹µë¥  =  0.86488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leedonghyeok/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "reviews_train = load_files(\"/Users/leedonghyeok/Downloads/aclImdb/train\")\n",
    "reviews_test = load_files(\"/Users/leedonghyeok/Downloads/aclImdb/test\")\n",
    "\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "\n",
    "print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ìˆ˜: {}\".format(len(text_test)))\n",
    "print(\"í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ (í…ŒìŠ¤íŠ¸ ë°ì´í„°): {}\".format(np.bincount(y_test)))\n",
    "\n",
    "text_train = [doc.replace(b\"<br />\", b\" \")  for doc in text_train]\n",
    "text_test = [doc.replace(b\"<br />\", b\" \")  for doc in text_test]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "vect = CountVectorizer(tokenizer = tokenize, stop_words = 'english').fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "X_test = vect.transform(text_test)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "pre = clf.predict(X_test)\n",
    "\n",
    "ac_score = accuracy_score(y_test,pre)\n",
    "print(\"ì •ë‹µë¥  = \", ac_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuray_score : maximum iteration íšŸìˆ˜ê°€ ì¡´ì¬í•˜ì—¬ ëŒë¦¬ë©´ ëŒë¦´ìˆ˜ë¡ ìˆ˜ì¹˜ê°€ ì˜¬ë¼ê°ˆ ë“¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(x|y) >= P(x,y)\n",
    "# P(x,y) = P(x) P(y) (if x and y are statistically independent)\n",
    "# P(x,y) = P(y,x) -> P(x|y) = P(y|x) P(x) / P(y) : Bayesian theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
